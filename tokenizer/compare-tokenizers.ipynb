{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on whitespace 99 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['▁We ▁a im ed ▁to ▁evaluate ▁the ▁effect ▁of ▁sleep ▁quality ▁on ▁memory , ▁ex ec ut ive ▁function , ▁and ▁language ▁performance ▁in ▁patients ▁with ▁re frac t ory ▁fo cal ▁epi le p s y ▁and ▁control led ▁epi le p s y ▁and ▁comp are ▁these </s>', '▁with ▁health y ▁individual s . ▁We ▁pro spec t ive ly ▁en roll ed ▁ 37 ▁ ado les cent ▁and ▁adult ▁patients ▁with ▁re frac t ory ▁fo cal ▁epi le p s y . </s>', '</s>', '▁How ▁to ▁avoid ▁anti - c lock wise ▁ro t ation ▁an im ation ▁when ▁re set ing ▁ro t ation ▁from ▁3 60 de g ▁to ▁0 ▁de g ? </s>', '</s>', '▁I ▁am ▁cre ating ▁an ▁an im ation ▁that ▁look s ▁like ▁a ▁f ancy ▁wh e el , ▁When ▁re set ting ▁ro t ation ▁from ▁3 60 de g ▁to ▁0 ▁de g , ▁It ▁an im ating ▁the ▁wh e el ▁in ▁anti - c lock wise ▁direction , ▁How ▁to ▁A vo id ▁this ? ? ? </s>', '▁H T M L </s>', '▁< ul ▁class =\" cm \"> </s>', '▁< li >< span > 01 </ span ></ li > </s>', '▁< li >< span > 02 </ span ></ li > </s>', '▁< li >< span > 03 </ span ></ li > </s>', '▁< li >< span > 04 </ span ></ li > </s>', '▁< li >< span > 05 </ span ></ li > </s>', '▁< li >< span > 06 </ span ></ li > </s>', '▁< li >< span > 07 </ span ></ li > </s>', '▁< li >< span > 08 </ span ></ li > </s>', '</s>', '▁</ ul > </s>', '</s>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['▁We ▁aim ed ▁to ▁evaluate ▁the ▁effect ▁of ▁sleep ▁quality ▁on ▁memory , ▁executive ▁function , ▁and ▁language ▁performance ▁in ▁patients ▁with ▁re frac tory ▁fo cal ▁epi le p s y ▁and ▁controlled ▁epi le p s y ▁and ▁compare ▁these </s>', '▁with ▁healthy ▁individuals . ▁We ▁prospect ive ly ▁en roll ed ▁37 ▁ ado les cent ▁and ▁adult ▁patients ▁with ▁re frac tory ▁fo cal ▁epi le p s y . </s>', '</s>', '▁How ▁to ▁avoid ▁anti - c lock wise ▁rotation ▁ anim ation ▁when ▁re set ing ▁rotation ▁from ▁3 60 de g ▁to ▁0 ▁de g ? </s>', '</s>', '▁I ▁am ▁creating ▁an ▁ anim ation ▁that ▁looks ▁like ▁a ▁f ancy ▁wheel , ▁When ▁res etting ▁rotation ▁from ▁3 60 de g ▁to ▁0 ▁de g , ▁It ▁ anim ating ▁the ▁wheel ▁in ▁anti - c lock wise ▁direction , ▁How ▁to ▁A void ▁this ? ? ? </s>', '▁ HTML </s>', '▁< ul ▁class =\" cm \"> </s>', '▁< li >< span > 01 </ span ></ li > </s>', '▁< li >< span > 02 </ span ></ li > </s>', '▁< li >< span > 03 </ span ></ li > </s>', '▁< li >< span > 04 </ span ></ li > </s>', '▁< li >< span > 05 </ span ></ li > </s>', '▁< li >< span > 06 </ span ></ li > </s>', '▁< li >< span > 07 </ span ></ li > </s>', '▁< li >< span > 08 </ span ></ li > </s>', '</s>', '▁</ ul > </s>', '</s>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['▁We ▁aimed ▁to ▁evaluate ▁the ▁effect ▁of ▁sleep ▁quality ▁on ▁memory , ▁executive ▁function , ▁and ▁language ▁performance ▁in ▁patients ▁with ▁re frac tory ▁focal ▁epi le psy ▁and ▁controlled ▁epi le psy ▁and ▁compare ▁these </s>', '▁with ▁healthy ▁individuals . ▁We ▁prospective ly ▁enrolled ▁37 ▁adolescent ▁and ▁adult ▁patients ▁with ▁re frac tory ▁focal ▁epi le psy . </s>', '</s>', '▁How ▁to ▁avoid ▁anti - clock wise ▁rotation ▁animation ▁when ▁reset ing ▁rotation ▁from ▁ 360 de g ▁to ▁0 ▁de g ? </s>', '</s>', '▁I ▁am ▁creating ▁an ▁animation ▁that ▁looks ▁like ▁a ▁fancy ▁wheel , ▁When ▁reset ting ▁rotation ▁from ▁ 360 de g ▁to ▁0 ▁de g , ▁It ▁ anim ating ▁the ▁wheel ▁in ▁anti - clock wise ▁direction , ▁How ▁to ▁A void ▁this ??? </s>', '▁HTML </s>', '▁< ul ▁class =\" cm \"> </s>', '▁< li >< span > 01 </ span ></ li > </s>', '▁< li >< span > 02 </ span ></ li > </s>', '▁< li >< span > 03 </ span ></ li > </s>', '▁< li >< span > 04 </ span ></ li > </s>', '▁< li >< span > 05 </ span ></ li > </s>', '▁< li >< span > 06 </ span ></ li > </s>', '▁< li >< span > 07 </ span ></ li > </s>', '▁< li >< span > 08 </ span ></ li > </s>', '</s>', '▁</ ul > </s>', '</s>']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['▁We ▁aimed ▁to ▁evaluate ▁the ▁effect ▁of ▁sleep ▁quality ▁on ▁memory , ▁executive ▁function , ▁and ▁language ▁performance ▁in ▁patients ▁with ▁refractory ▁focal ▁epilepsy ▁and ▁controlled ▁epilepsy ▁and ▁compare ▁these </s>', '▁with ▁healthy ▁individuals . ▁We ▁prospective ly ▁enrolled ▁37 ▁adolescent ▁and ▁adult ▁patients ▁with ▁refractory ▁focal ▁epilepsy . </s>', '</s>', '▁How ▁to ▁avoid ▁anti - clockwise ▁rotation ▁animation ▁when ▁reset ing ▁rotation ▁from ▁360 deg ▁to ▁0 ▁de g ? </s>', '</s>', '▁I ▁am ▁creating ▁an ▁animation ▁that ▁looks ▁like ▁a ▁fancy ▁wheel , ▁When ▁re setting ▁rotation ▁from ▁360 deg ▁to ▁0 ▁de g , ▁It ▁ anim ating ▁the ▁wheel ▁in ▁anti - clockwise ▁direction , ▁How ▁to ▁Avoid ▁this ??? </s>', '▁HTML </s>', '▁< ul ▁class =\" cm \"> </s>', '▁< li >< span > 01 </ span ></ li > </s>', '▁< li >< span > 02 </ span ></ li > </s>', '▁< li >< span > 03 </ span ></ li > </s>', '▁< li >< span > 04 </ span ></ li > </s>', '▁< li >< span > 05 </ span ></ li > </s>', '▁< li >< span > 06 </ span ></ li > </s>', '▁< li >< span > 07 </ span ></ li > </s>', '▁< li >< span > 08 </ span ></ li > </s>', '</s>', '▁</ ul > </s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import sentencepiece as spm\n",
    "from tokenizers import SentencePieceUnigramTokenizer, Tokenizer\n",
    "\n",
    "text_sample = \"\"\"We aimed to evaluate the effect of sleep quality on memory, executive function, and language performance in patients with refractory focal epilepsy and controlled epilepsy and compare these\n",
    " with healthy individuals. We prospectively enrolled 37 adolescent and adult patients with refractory focal epilepsy.\n",
    "\n",
    "How to avoid anti-clockwise rotation animation when reseting rotation from 360deg to 0 deg?\n",
    "\n",
    "I am creating an animation that looks like a fancy wheel, When resetting rotation from 360deg to 0 deg, It animating the wheel in anti-clockwise direction, How to Avoid this???\n",
    "HTML\n",
    "<ul class=\"cm\">\n",
    "  <li><span>01</span></li>\n",
    "  <li><span>02</span></li>\n",
    "  <li><span>03</span></li>\n",
    "  <li><span>04</span></li>\n",
    "  <li><span>05</span></li>\n",
    "  <li><span>06</span></li>\n",
    "  <li><span>07</span></li>\n",
    "  <li><span>08</span></li>\n",
    "\n",
    "</ul>\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "print(\"Split on whitespace\", len(text_sample.split()), \"tokens\")\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "#     print(f\"Loading {t}k_model\")\n",
    "#     tokenizer = Tokenizer.from_file(f\"tokenizers/pile_{t}.json\")\n",
    "#     tokenized = tokenizer.encode(text_sample)\n",
    "#     print(len(tokenized), \"tokens\")\n",
    "#     print(\"-\" *100)\n",
    "#     print(\" \".join(tokenized.tokens))\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file=f'spmodels/pile_{t}k.model', add_eos=True)\n",
    "    \n",
    "    tokenized = sp.encode(text_sample.splitlines(), out_type=str)\n",
    "    print(len(tokenized), \"tokens\")\n",
    "    print(\"-\" *100)\n",
    "    print(list(map(\" \".join, tokenized)))\n",
    "    \n",
    "\n",
    "# print(\"-\" *100)\n",
    "# print(\"Albert-tokenizer\")\n",
    "# tok = AlbertTokenizer.from_pretrained(\"albert-large-v2\")\n",
    "# altok = tok.encode(text_sample)\n",
    "# print(len(altok), \"tokens\")\n",
    "# print(\"-\" *100)\n",
    "# print(\" \".join(tok.convert_ids_to_tokens(altok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save tokenizers into json\"\"\"\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading {t}k_model\")\n",
    "    tokenizer = SentencePieceUnigramTokenizer.from_spm(f\"spmodels/pile_{t}k.model\", add_eos=True)\n",
    "    tokenizer.save(f\"tokenizers/pile_{t}.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 4k_model\n",
      "34051862 tokens\n",
      "processed in  29 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 8k_model\n",
      "30989028 tokens\n",
      "processed in  32 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 16k_model\n",
      "26695247 tokens\n",
      "processed in  21 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 32k_model\n",
      "24625391 tokens\n",
      "processed in  21 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 4k_model\n",
      "35069053 tokens\n",
      "processed in  24 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 8k_model\n",
      "32006219 tokens\n",
      "processed in  23 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 16k_model\n",
      "27712438 tokens\n",
      "processed in  22 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 32k_model\n",
      "25642579 tokens\n",
      "processed in  22 secs\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "text_sample = open(\"../data/enwik8/train.txt.raw\").read().splitlines()\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading tokenizers {t}k_model\")\n",
    "    tokenizer = Tokenizer.from_file(f\"tokenizers/pile_{t}.json\")\n",
    "    \n",
    "    starttime = time.time()\n",
    "    tokenized = tokenizer.encode_batch(text_sample, add_special_tokens=True)\n",
    "    \n",
    "    print(sum(len(x) for x in tokenized), \"tokens\")\n",
    "    print(\"processed in \", int(time.time() - starttime), \"secs\")\n",
    "    print(\"-\" *100)\n",
    "    \n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading spm {t}k_model\")\n",
    "    sp = spm.SentencePieceProcessor(model_file=f'spmodels/pile_{t}k.model', add_eos=True)\n",
    "\n",
    "    starttime = time.time()\n",
    "    sptokenized = sp.encode(text_sample)\n",
    "    \n",
    "    print(sum(len(x) for x in sptokenized), \"tokens\")\n",
    "    print(\"processed in \", int(time.time() - starttime), \"secs\")\n",
    "    print(\"-\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train\n",
      "Tokenizing valid\n",
      "Tokenizing test\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import chain\n",
    "\n",
    "sp32 = spm.SentencePieceProcessor(model_file=f'spmodels/pile_32k.model', add_eos=True)\n",
    "enwik8 = []\n",
    "\n",
    "for s in (\"train\", \"valid\", \"test\"):\n",
    "    print(\"Tokenizing\", s)\n",
    "    text_sample = open(f\"../data/enwik8/{s}.txt.raw\").read().splitlines()\n",
    "    tokenized_text = sp32.encode(text_sample)\n",
    "    data = torch.tensor(list(chain(*tokenized_text)), dtype=torch.int)\n",
    "    enwik8.append(data)\n",
    "\n",
    "torch.save(enwik8, f\"../data/enwik8_sp/enwik8.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
