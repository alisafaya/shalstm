{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on whitespace 99 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[292, 1924, 2], [158, 40, 232, 2]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[233, 1431, 2], [137, 52, 400, 2]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[192, 1163, 2], [4978, 452, 2]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2 tokens\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[[173, 1111, 2], [3908, 710, 2]]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import sentencepiece as spm\n",
    "from tokenizers import SentencePieceUnigramTokenizer, Tokenizer\n",
    "\n",
    "text_sample = \"\"\"We aimed to evaluate the effect of sleep quality on memory, executive function, and language performance in patients with refractory focal epilepsy and controlled epilepsy and compare these\n",
    " with healthy individuals. We prospectively enrolled 37 adolescent and adult patients with refractory focal epilepsy.\n",
    "\n",
    "How to avoid anti-clockwise rotation animation when reseting rotation from 360deg to 0 deg?\n",
    "\n",
    "I am creating an animation that looks like a fancy wheel, When resetting rotation from 360deg to 0 deg, It animating the wheel in anti-clockwise direction, How to Avoid this???\n",
    "HTML\n",
    "<ul class=\"cm\">\n",
    "  <li><span>01</span></li>\n",
    "  <li><span>02</span></li>\n",
    "  <li><span>03</span></li>\n",
    "  <li><span>04</span></li>\n",
    "  <li><span>05</span></li>\n",
    "  <li><span>06</span></li>\n",
    "  <li><span>07</span></li>\n",
    "  <li><span>08</span></li>\n",
    "\n",
    "</ul>\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "print(\"Split on whitespace\", len(text_sample.split()), \"tokens\")\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "#     print(f\"Loading {t}k_model\")\n",
    "#     tokenizer = Tokenizer.from_file(f\"tokenizers/pile_{t}.json\")\n",
    "#     tokenized = tokenizer.encode(text_sample)\n",
    "#     print(len(tokenized), \"tokens\")\n",
    "#     print(\"-\" *100)\n",
    "#     print(\" \".join(tokenized.tokens))\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file=f'spmodels/pile_{t}k.model', add_eos=True)\n",
    "    \n",
    "    tokenized = sp.encode([\"some text\", \"somte\"])\n",
    "#     print(text_sample.splitlines())\n",
    "    print(len(tokenized), \"tokens\")\n",
    "    print(\"-\" *100)\n",
    "#     print(list(map(\" \".join, tokenized)))\n",
    "    print(tokenized)\n",
    "\n",
    "# print(\"-\" *100)\n",
    "# print(\"Albert-tokenizer\")\n",
    "# tok = AlbertTokenizer.from_pretrained(\"albert-large-v2\")\n",
    "# altok = tok.encode(text_sample)\n",
    "# print(len(altok), \"tokens\")\n",
    "# print(\"-\" *100)\n",
    "# print(\" \".join(tok.convert_ids_to_tokens(altok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Loading 4k_model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading 8k_model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading 16k_model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading 32k_model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save tokenizers into json\"\"\"\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading {t}k_model\")\n",
    "    tokenizer = SentencePieceUnigramTokenizer.from_spm(f\"spmodels/pile_{t}k.model\")\n",
    "    tokenizer.save(f\"tokenizers/pile_{t}.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 4k_model\n",
      "34051862 tokens\n",
      "processed in  17 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 8k_model\n",
      "30989028 tokens\n",
      "processed in  20 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 16k_model\n",
      "26695247 tokens\n",
      "processed in  19 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading tokenizers 32k_model\n",
      "24625391 tokens\n",
      "processed in  18 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 4k_model\n",
      "34051862 tokens\n",
      "processed in  27 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 8k_model\n",
      "30989028 tokens\n",
      "processed in  28 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 16k_model\n",
      "26695247 tokens\n",
      "processed in  25 secs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 32k_model\n",
      "24625388 tokens\n",
      "processed in  25 secs\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "text_sample = open(\"../data/enwik8/train.txt.raw\").read().splitlines()\n",
    "\n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading tokenizers {t}k_model\")\n",
    "    tokenizer = Tokenizer.from_file(f\"tokenizers/pile_{t}.json\")\n",
    "    \n",
    "    starttime = time.time()\n",
    "    tokenized = tokenizer.encode_batch(text_sample, add_special_tokens=True)\n",
    "    \n",
    "    print(sum(len(x) for x in tokenized), \"tokens\")\n",
    "    print(\"processed in \", int(time.time() - starttime), \"secs\")\n",
    "    print(\"-\" *100)\n",
    "    \n",
    "for t in (4, 8, 16, 32):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Loading spm {t}k_model\")\n",
    "    sp = spm.SentencePieceProcessor(model_file=f'spmodels/pile_{t}k.model')\n",
    "\n",
    "    starttime = time.time()\n",
    "    sptokenized = sp.encode(text_sample)\n",
    "    \n",
    "    print(sum(len(x) for x in sptokenized), \"tokens\")\n",
    "    print(\"processed in \", int(time.time() - starttime), \"secs\")\n",
    "    print(\"-\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Loading spm 16k_model\n",
      "processed in  23 secs\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Save tokenizers into json\"\"\"\n",
    "\n",
    "import time\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "text_sample = open(\"../data/enwik8/train.txt.raw\").read().splitlines()\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(f\"Loading spm 16k_model\")\n",
    "sp = spm.SentencePieceProcessor(model_file=f'spmodels/pile_16k.model')\n",
    "\n",
    "starttime = time.time()\n",
    "sptokenized = sp.encode(text_sample)\n",
    "\n",
    "print(\"processed in \", int(time.time() - starttime), \"secs\")\n",
    "print(\"-\" *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "al = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26695247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
